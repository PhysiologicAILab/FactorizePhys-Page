<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header styles */
        header {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 15px 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 20px rgba(0, 0, 0, 0.1);
        }

        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 1.4rem;
            font-weight: 700;
            color: #667eea;
        }

        .nav-links {
            display: flex;
            list-style: none;
            gap: 1.5rem;
        }

        .nav-links a {
            text-decoration: none;
            color: #333;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .nav-links a:hover {
            color: #667eea;
        }

        /* Main content */
        main {
            margin-top: 80px;
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            margin-bottom: 30px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        }

        .hero {
            text-align: center;
            padding: 60px 40px;
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1));
            border-radius: 20px 20px 0 0;
        }

        .hero h1 {
            font-size: 2.5rem;
            margin-bottom: 20px;
            color: #2c3e50;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1rem;
            color: #5a6c7d;
            margin-bottom: 15px;
        }

        .conference {
            display: inline-block;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 12px 24px;
            border-radius: 25px;
            font-weight: 600;
            margin: 20px 0;
            font-size: 1.1rem;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
            margin-top: 30px;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 12px 24px;
            background: #fff;
            color: #333;
            text-decoration: none;
            border-radius: 25px;
            border: 2px solid #e9ecef;
            transition: all 0.3s ease;
            font-weight: 500;
        }

        .btn:hover {
            background: #667eea;
            color: white;
            border-color: #667eea;
            transform: translateY(-2px);
        }

        /* Section styles */
        .section {
            padding: 40px;
            margin-bottom: 30px;
        }

        .section h2 {
            color: #2c3e50;
            margin-bottom: 25px;
            font-size: 1.8rem;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        .section h3 {
            color: #34495e;
            margin-bottom: 15px;
            margin-top: 25px;
            font-size: 1.3rem;
        }

        .section h4 {
            color: #34495e;
            margin-bottom: 10px;
            margin-top: 20px;
            font-size: 1.1rem;
        }

        .highlights {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .highlight-card {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 15px;
            border-left: 4px solid #667eea;
            transition: transform 0.3s ease;
        }

        .highlight-card:hover {
            transform: translateY(-5px);
        }

        .highlight-card i {
            color: #667eea;
            font-size: 1.5rem;
            margin-bottom: 10px;
        }

        .highlight-card h3,
        .highlight-card h4 {
            margin-top: 0;
            margin-bottom: 10px;
        }

        /* Figure styles */
        .figure-container {
            margin: 30px 0;
            text-align: center;
        }

        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            cursor: pointer;
            transition: transform 0.3s ease;
        }

        .figure-container img:hover {
            transform: scale(1.02);
        }

        .figure-caption {
            margin-top: 15px;
            font-style: italic;
            color: #5a6c7d;
            line-height: 1.4;
        }

        /* Code block styles */
        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            position: relative;
        }

        .code-block::before {
            content: 'Terminal';
            position: absolute;
            top: 8px;
            right: 15px;
            font-size: 0.8rem;
            color: #a0aec0;
        }

        .code-block pre {
            margin: 0;
            white-space: pre-wrap;
        }

        .code-comment {
            color: #68d391;
        }

        /* Mathematical content styles */
        .math-section {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }

        .equation-block {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }

        /* Performance metrics */
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .metric-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            border: 2px solid #e9ecef;
        }

        .metric-value {
            font-size: 2rem;
            font-weight: bold;
            color: #667eea;
            display: block;
        }

        .metric-label {
            color: #5a6c7d;
            margin-top: 5px;
        }

        /* Table styles */
        .table-container {
            background: white;
            border-radius: 10px;
            overflow: hidden;
            margin: 30px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .table-container table {
            width: 100%;
            border-collapse: collapse;
        }

        .table-container th,
        .table-container td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }

        .table-container th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        .table-container tr:hover {
            background: #f8f9fa;
        }

        /* Citation box */
        .citation-box {
            background: #2d3748;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
        }

        .citation-box h2 {
            color: #68d391;
            margin-bottom: 15px;
            font-family: 'Segoe UI', sans-serif;
        }

        /* References */
        .references {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
        }

        .references ol {
            padding-left: 20px;
        }

        .references li {
            margin-bottom: 10px;
            line-height: 1.6;
        }

        /* Key insight box */
        .key-insight {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1));
            padding: 25px;
            border-radius: 15px;
            margin: 30px 0;
            border: 2px solid rgba(102, 126, 234, 0.2);
        }

        .key-insight h3 {
            color: #667eea;
            margin-top: 0;
        }

        /* Responsive design */
        @media (max-width: 768px) {
            .hero h1 {
                font-size: 1.8rem;
            }

            .buttons {
                flex-direction: column;
                align-items: center;
            }

            .nav-links {
                display: none;
            }

            .section {
                padding: 20px;
            }

            .highlights {
                grid-template-columns: 1fr;
            }
        }

        /* Animation */
        .animated {
            opacity: 0;
            transform: translateY(30px);
            animation: slideUp 0.8s ease forwards;
        }

        @keyframes slideUp {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>

<body>
    <header>
        <nav class="container">
            <div class="logo">FactorizePhys</div>
            <ul class="nav-links">
                <li><a href="#overview">Overview</a></li>
                <li><a href="#technical-details">Technical Details</a></li>
                <li><a href="#performance">Performance</a></li>
                <li><a href="#code">Code & Data</a></li>
                <li><a href="#references">References</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section class="hero animated">
            <h1>FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing</h1>

            <div class="authors">
                <strong>Jitesh Joshi</strong><sup>1</sup>, <strong>Sos S. Agaian</strong><sup>2</sup>, <strong>Youngjun
                    Cho</strong><sup>1</sup>
                <br>
                <sup>1</sup>University College London, UK • <sup>2</sup>City University of New York, USA
            </div>

            <div class="conference">
                <i class="fas fa-trophy"></i> NeurIPS 2024
            </div>

            <div class="buttons">
                <a href="https://github.com/PhysiologicAILab/FactorizePhys" class="btn">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="assets/docs/factorizephys-paper.pdf" class="btn">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/af1c61e4dd59596f033d826419870602-Abstract-Conference.html"
                    class="btn">
                    <i class="fas fa-book"></i> NeurIPS Proceedings
                </a>
            </div>
        </section>

        <section id="overview" class="section animated">
            <h2><i class="fas fa-eye"></i> Overview</h2>

            <p>FactorizePhys introduces a breakthrough in remote photoplethysmography (rPPG) through the novel
                <strong>Factorized Self-Attention Module (FSAM)</strong>, which leverages nonnegative matrix
                factorization to jointly compute multidimensional attention across spatial, temporal, and channel
                dimensions. This work demonstrates that matrix factorization can effectively serve as a multidimensional
                attention mechanism, outperforming traditional transformer-based approaches for remote physiological
                sensing.</p>

            <div class="figure-container">
                <img src="assets/images/paper-figures/FactorizePhysPoster.png"
                    alt="FactorizePhys Poster at NeurIPS 2024">
                <p class="figure-caption"><strong>Figure 1:</strong> Our poster at NeurIPS 2024, where we discussed FSAM
                    with fellow researchers from the computer vision and machine learning community.</p>
            </div>

            <div class="highlights">
                <div class="highlight-card">
                    <i class="fas fa-brain"></i>
                    <h3>Novel Attention Mechanism</h3>
                    <p>FSAM jointly computes spatial-temporal-channel attention using matrix factorization, unlike
                        existing methods that compute attention disjointly.</p>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-rocket"></i>
                    <h3>Superior Generalization</h3>
                    <p>Achieves state-of-the-art cross-dataset performance, demonstrating robust generalization across
                        different rPPG datasets.</p>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-tachometer-alt"></i>
                    <h3>Computational Efficiency</h3>
                    <p>Approximately 50× fewer parameters than existing methods while maintaining competitive
                        performance.</p>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-chart-line"></i>
                    <h3>Exceptional Performance</h3>
                    <p>67% reduction in Mean Absolute Error (MAE) and 15% improvement in Signal-to-Noise Ratio (SNR)
                        compared to state-of-the-art methods.</p>
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <span class="metric-value">67%</span>
                    <div class="metric-label">MAE Reduction</div>
                </div>
                <div class="metric-card">
                    <span class="metric-value">15%</span>
                    <div class="metric-label">SNR Improvement</div>
                </div>
                <div class="metric-card">
                    <span class="metric-value">51K</span>
                    <div class="metric-label">Parameters</div>
                </div>
                <div class="metric-card">
                    <span class="metric-value">0.998</span>
                    <div class="metric-label">HR Correlation</div>
                </div>
            </div>
        </section>

        <section id="technical-details" class="section animated">
            <h2><i class="fas fa-cogs"></i> Technical Details</h2>

            <h3>The Compression-as-Attention Paradigm</h3>
            <p>Using compression as an attention mechanism isn't new. In the CNN-dominated era,
                <strong>squeeze-and-excitation (SE) attention</strong> [<a href="#ref2">2</a>] was among the most
                popular mechanisms. However, a fundamental limitation emerges when working with multidimensional feature
                spaces: <strong>existing attention mechanisms compute attention disjointly across spatial, temporal, and
                    channel dimensions</strong>.</p>

            <p>For tasks like rPPG estimation that require joint modeling of these dimensions, squeezing individual
                dimensions can result in information loss, causing learned attention to miss comprehensive
                multidimensional feature relationships.</p>

            <h3>FSAM: Factorized Self-Attention Module</h3>
            <p>FSAM uses Non-negative Matrix Factorization (NMF) [<a href="#ref3">3</a>] to factorize multidimensional
                feature space into a low-rank approximation, serving as a compressed representation that preserves
                interdependencies across all dimensions.</p>

            <div class="figure-container">
                <img src="assets/images/paper-figures/FSAM.png" alt="FSAM Overview" style="width:60%; height:auto;">
                <p class="figure-caption"><strong>Figure 2:</strong> Overview of the Factorized Self-Attention Module
                    (FSAM) showing how multidimensional voxel embeddings are transformed into a 2D matrix, factorized
                    using NMF, and reconstructed to provide attention weights.</p>
            </div>

            <div class="highlights">
                <div class="highlight-card">
                    <h4>1. Joint Multidimensional Attention</h4>
                    <p>No dimension squeezing required; processes spatial, temporal, and channel dimensions
                        simultaneously</p>
                </div>

                <div class="highlight-card">
                    <h4>2. Parameter-free Optimization</h4>
                    <p>Uses classic Lee & Seung multiplicative update rules [<a href="#ref3">3</a>] implemented under
                        'no_grad' block</p>
                </div>

                <div class="highlight-card">
                    <h4>3. Task-specific Design</h4>
                    <p>Tailored for signal extraction tasks with rank-1 factorization</p>
                </div>
            </div>

            <div class="figure-container">
                <img src="assets/images/paper-figures/FactorizePhysFSAMEfficientPhys.png"
                    alt="FactorizePhys Architecture">
                <p class="figure-caption"><strong>Figure 3:</strong> FactorizePhys Architecture. (A) Proposed
                    FactorizePhys with FSAM; (B) FSAM Adapted for EfficientPhys showing 3D-CNN architecture and 2D-CNN
                    adaptation.</p>
            </div>

            <div class="math-section">
                <h3>Mathematical Formulation</h3>

                <h4>The Critical Transformation</h4>
                <p>Input spatial-temporal data can be expressed as $\mathcal{I} \in \mathbb{R}^{T \times C \times H
                    \times W}$, where $T$, $C$, $H$, and $W$ represent total frames (temporal dimension), channels in a
                    frame (e.g., for RGB frames, $C=3$), height and width of pixels in a frame, respectively. For this
                    input $\mathcal{I}$, we generate voxel embeddings $\varepsilon \in \mathbb{R}^{\tau \times \kappa
                    \times \alpha \times \beta}$ through 3D feature extraction.</p>

                <p><strong>Traditional 2D approach</strong> (like Hamburger module [<a href="#ref4">4</a>]):</p>
                <div class="equation-block">
                    $$V^{s} \in \mathbb{R}^{M \times N} = \Gamma^{\kappa\alpha\beta \mapsto MN}(\xi_{pre}(\varepsilon
                    \in \mathbb{R}^{\kappa \times \alpha \times \beta})) \ni \kappa \mapsto M,\alpha \times \beta
                    \mapsto N$$
                </div>

                <p><strong>Our 3D spatial-temporal approach</strong>:</p>
                <div class="equation-block">
                    $$V^{st} \in \mathbb{R}^{M \times N} = \Gamma^{\tau\kappa\alpha\beta \mapsto
                    MN}(\xi_{pre}(\varepsilon \in \mathbb{R}^{\tau \times \kappa \times \alpha \times \beta})) \ni \tau
                    \mapsto M, \kappa \times \alpha \times \beta \mapsto N$$
                </div>

                <p>This transformation is <strong>crucial</strong> for rPPG estimation because:</p>
                <ul>
                    <li><strong>Physiological signal correlation</strong>: We need correlations between spatial/channel
                        features and temporal patterns for BVP signal recovery</li>
                    <li><strong>Single signal source</strong>: Only one underlying BVP signal across facial regions
                        justifies rank-1 factorization ($L=1$)</li>
                    <li><strong>Scale considerations</strong>: Temporal and spatial dimensions have vastly different
                        scales</li>
                </ul>
            </div>

            <h3>The NMF Attention Mechanism</h3>
            <p>The factorization process uses iterative multiplicative updates:</p>

            <div class="code-block">
                <pre><code>def fsam(E):
    S = 1
    rank = 1
    MD_STEPS = 4
    ε = 1e-4
    batch, channel_dim, temporal_dim, alpha, beta = E.shape
    spatial_dim = alpha * beta

    <span class="code-comment"># Preprocessing: ensure non-negativity for NMF</span>
    x = ReLU(Conv3D(E - E.min()))

    <span class="code-comment"># Transform to factorization matrix</span>
    V_st = reshape(x, (batch * S, temporal_dim, spatial_dim * channel_dim))

    <span class="code-comment"># Initialize bases and coefficients</span>
    bases = torch.ones(batch * S, temporal_dim, rank)
    coef = softmax(V_st.transpose(-2, -1) @ bases)

    <span class="code-comment"># Iterative multiplicative updates (4-8 steps)</span>
    for step in range(MD_STEPS):
        <span class="code-comment"># Update coefficients</span>
        numerator = V_st.transpose(-2, -1) @ bases
        denominator = coef @ (bases.transpose(-2, -1) @ bases)
        coef = coef * (numerator / (denominator + ε))
        
        <span class="code-comment"># Update bases</span>
        numerator = V_st @ coef
        denominator = bases @ (coef.transpose(-2, -1) @ coef)
        bases = bases * (numerator / (denominator + ε))

    <span class="code-comment"># Reconstruct attention</span>
    V̂_st = bases @ coef.transpose(-2, -1)
    Ê = reshape(V̂_st, (batch, channel_dim, temporal_dim, alpha, beta))

    <span class="code-comment"># Apply attention with residual connection</span>
    output = E + InstanceNorm(E * ReLU(Conv3D(Ê)))
    
    return output</code></pre>
            </div>

            <h3>Why Rank-1 Factorization Works</h3>
            <p>The paper's ablation studies confirm that <strong>rank-1 factorization performs optimally</strong> for
                rPPG estimation. This aligns with the physiological assumption that there's a single underlying blood
                volume pulse signal across different facial regions.</p>

            <div class="figure-container">
                <img src="assets/images/paper-figures/rank-ablation.png" alt="Rank Ablation Study"
                    style="width:70%; height:auto;">
                <p class="figure-caption"><strong>Table 1:</strong> Ablation study results showing performance across
                    different factorization ranks. Rank-1 achieves optimal performance, supporting the single signal
                    source assumption for rPPG estimation.</p>
            </div>
        </section>

        <section id="performance" class="section animated">
            <h2><i class="fas fa-chart-bar"></i> Performance Analysis</h2>

            <h3>Why FSAM Outperforms Transformers</h3>

            <h4>1. Task-Specific vs Generic Design</h4>
            <p><strong>Transformers</strong> use generic self-attention that treats all positions equally:</p>
            <div class="equation-block">
                $$Attention(Q,K,V) = softmax(QK^T/\sqrt{d_k})V$$
            </div>

            <p><strong>FSAM</strong> is specifically designed for spatial-temporal signal extraction:</p>
            <ul>
                <li><strong>Temporal vectors as the primary dimension</strong> (signals evolve over time, directly
                    supervised through loss function)</li>
                <li><strong>Spatial-channel features as descriptors</strong> (different facial regions contribute
                    differently)</li>
                <li><strong>Rank-1 constraint</strong> enforces single signal source assumption</li>
            </ul>

            <h4>2. Computational Efficiency</h4>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Complexity</th>
                            <th>Parameters</th>
                            <th>Update Steps</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>FSAM</strong></td>
                            <td>O(n)</td>
                            <td>52K</td>
                            <td>4-8 multiplicative updates</td>
                        </tr>
                        <tr>
                            <td>Transformer [<a href="#ref5">5</a>]</td>
                            <td>O(n²)</td>
                            <td>7.38M</td>
                            <td>Full attention computation</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="key-insight">
                <h3>Efficiency Breakthrough</h3>
                <p><strong>138× fewer parameters</strong> compared to PhysFormer [<a href="#ref6">6</a>] while achieving
                    superior performance across all evaluation metrics.</p>
            </div>

            <h3>Cross-Dataset Generalization Performance</h3>
            <p>FactorizePhys demonstrates superior performance across all evaluation metrics when tested on unseen
                datasets:</p>

            <div class="figure-container">
                <img src="assets/images/paper-figures/cross-dataset-performance.png"
                    alt="Cross-dataset Performance Evaluation">
                <p class="figure-caption"><strong>Table 2:</strong> Cross-dataset Performance Evaluation. Comprehensive
                    comparison showing FactorizePhys outperforming SOTA methods across all datasets with MAE, RMSE,
                    MAPE, Correlation, SNR, and MACC metrics.</p>
            </div>

            <h3>Performance vs. Latency Analysis</h3>
            <div class="figure-container">
                <img src="assets/images/paper-figures/InferenceLatency.png" alt="Performance vs. Latency">
                <p class="figure-caption"><strong>Figure 4:</strong> Performance vs. Latency. Cumulative cross-dataset
                    performance (MAE) vs. latency plot with model parameters as sphere size, showing FactorizePhys
                    achieving best performance with fewer parameters.</p>
            </div>

            <h3>Attention Visualization</h3>
            <p>Our cosine similarity visualization between temporal embeddings and ground-truth PPG signals reveals
                higher correlation scores and better spatial selectivity for FactorizePhys with FSAM.</p>

            <div class="figure-container">
                <img src="assets/images/paper-figures/AttentionMaps.png" alt="Attention Visualization">
                <p class="figure-caption"><strong>Figure 5:</strong> Attention visualization comparing baseline model
                    (left) and FactorizePhys with FSAM (right). Higher cosine similarity scores (brighter regions)
                    indicate better spatial selectivity for pulse-rich facial regions.</p>
            </div>

            <h3>The Inference-Time Advantage</h3>
            <p>A remarkable finding: <strong>FactorizePhys trained with FSAM retains performance even when FSAM is
                    dropped during inference</strong>. This suggests FSAM acts as a "teacher" during training, guiding
                the network to learn more salient feature representations that persist without the attention module.</p>

            <div class="code-block">
                <pre><code><span class="code-comment"># Training: FSAM influences 3D convolutional kernels</span>
factorized_embeddings = fsam(voxel_embeddings)
loss = compute_loss(head(factorized_embeddings), ground_truth)

<span class="code-comment"># Inference: Can drop FSAM without performance loss</span>
output = head(voxel_embeddings)  <span class="code-comment"># No FSAM needed!</span></code></pre>
            </div>

            <p>This dramatically reduces inference latency while maintaining accuracy - ideal for real-time
                applications.</p>

            <h3>Key Contributions and Broader Implications</h3>

            <h4>For the rPPG Community:</h4>
            <ul>
                <li><strong>First matrix factorization-based attention</strong> specifically designed for physiological
                    signal extraction</li>
                <li><strong>State-of-the-art cross-dataset generalization</strong> with dramatically fewer parameters
                </li>
                <li><strong>Real-time deployment capability</strong> without performance degradation</li>
            </ul>

            <h4>For the Broader AI Community:</h4>
            <ul>
                <li><strong>Novel attention paradigm</strong> demonstrating that domain-specific designs can outperform
                    generic mechanisms</li>
                <li><strong>Efficiency breakthrough</strong>: 138× parameter reduction compared to transformers with
                    superior performance</li>
                <li><strong>New perspective on attention</strong>: Factorization as compression can be more effective
                    than dimension squeezing</li>
            </ul>

            <h3>Future Research Directions</h3>
            <p>FSAM's success opens several promising avenues:</p>
            <ol>
                <li><strong>Extended Applications</strong>: Video understanding, action recognition, medical time-series
                    analysis</li>
                <li><strong>Enhanced NMF Variants</strong>: Incorporating temporal smoothness or frequency domain
                    constraints</li>
                <li><strong>Multi-physiological Signals</strong>: Check out our subsequent work (<a
                        href="https://arxiv.org/abs/2505.07013" target="_blank">MMRPhys</a>) that explores this
                    direction for robust estimation of multiple physiological signals</li>
                <li><strong>Hybrid Architectures</strong>: Combining factorization-based attention with other mechanisms
                    for different modalities</li>
                <li><strong>Theoretical Analysis</strong>: Understanding why rank-1 factorization generalizes so well
                    across datasets</li>
            </ol>

            <div class="key-insight">
                <h3>Conclusion</h3>
                <p>FSAM demonstrates that deeper understanding of the problem domain can lead to more effective
                    solutions than applying generic, computationally expensive methods. In an era of ever-growing model
                    sizes, this work shows that <strong>thoughtful design trumps brute-force scaling</strong>.</p>
            </div>
        </section>

        <section id="code" class="section animated">
            <h2><i class="fab fa-github"></i> Code & Resources</h2>

            <div class="highlights">
                <div class="highlight-card">
                    <i class="fab fa-github"></i>
                    <h3>GitHub Repository</h3>
                    <p>Complete implementation with training scripts, evaluation metrics, and pre-trained models.</p>
                    <a href="https://github.com/PhysiologicAILab/FactorizePhys" class="btn" style="margin-top: 15px;">
                        <i class="fab fa-github"></i> View Repository
                    </a>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-toolbox"></i>
                    <h3>rPPG-Toolbox Integration</h3>
                    <p>Built on the comprehensive rPPG-Toolbox framework for fair comparison and reproducible results.
                    </p>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-database"></i>
                    <h3>Datasets</h3>
                    <p>Evaluated on iBVP, PURE, UBFC-rPPG, and SCAMPS datasets with standardized preprocessing.</p>
                </div>

                <div class="highlight-card">
                    <i class="fas fa-graduation-cap"></i>
                    <h3>Reproducible Research</h3>
                    <p>Complete experimental setup, hyperparameters, and evaluation protocols for reproducible results.
                    </p>
                </div>
            </div>

            <h3>Installation and Quick Start</h3>
            <div class="code-block">
                <pre><code><span class="code-comment"># Clone the repository</span>
git clone https://github.com/PhysiologicAILab/FactorizePhys.git
cd FactorizePhys

<span class="code-comment"># Install dependencies</span>
pip install -r requirements.txt

<span class="code-comment"># Train FactorizePhys with FSAM</span>
python train.py --model FactorizePhys --attention FSAM --dataset UBFC-rPPG

<span class="code-comment"># Evaluate model performance</span>
python evaluate.py --model FactorizePhys --dataset PURE --checkpoint checkpoints/best_model.pth

<span class="code-comment"># Run cross-dataset evaluation</span>
python cross_dataset_eval.py --train_dataset UBFC-rPPG --test_dataset iBVP</code></pre>
            </div>

            <div class="table-container">
                <h3>Code and Data Availability</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Resource</th>
                            <th>Description</th>
                            <th>Link</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Paper</strong></td>
                            <td>FactorizePhys: Matrix Factorization for Multidimensional Attention</td>
                            <td><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/af1c61e4dd59596f033d826419870602-Abstract-Conference.html"
                                    target="_blank">NeurIPS 2024</a></td>
                        </tr>
                        <tr>
                            <td><strong>Code</strong></td>
                            <td>Complete implementation and training scripts</td>
                            <td><a href="https://github.com/PhysiologicAILab/FactorizePhys" target="_blank">GitHub
                                    Repository</a></td>
                        </tr>
                        <tr>
                            <td><strong>Dataset</strong></td>
                            <td>iBVP Dataset for rPPG research</td>
                            <td><a href="https://github.com/PhysiologicAILab/iBVP-Dataset" target="_blank">iBVP
                                    Dataset</a></td>
                        </tr>
                        <tr>
                            <td><strong>Follow-up Work</strong></td>
                            <td>MMRPhys: Multi-modal physiological signal estimation</td>
                            <td><a href="https://arxiv.org/abs/2505.07013" target="_blank">arXiv:2505.07013</a></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="citation" class="section animated">
            <div class="citation-box">
                <h2><i class="fas fa-quote-left"></i> Citation</h2>
                <pre>@inproceedings{joshi2024factorizephys,
  author = {Joshi, Jitesh and Agaian, Sos S. and Cho, Youngjun},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages = {96607--96639},
  publisher = {Curran Associates, Inc.},
  title = {FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/af1c61e4dd59596f033d826419870602-Paper-Conference.pdf},
  volume = {37},
  year = {2024}
}</pre>
            </div>
        </section>

        <section id="references" class="section animated">
            <div class="references">
                <h2><i class="fas fa-book"></i> References</h2>
                <ol>
                    <li id="ref1">Joshi, J., Agaian, S. S., & Cho, Y. (2024). FactorizePhys: Matrix Factorization for
                        Multidimensional Attention in Remote Physiological Sensing. In <em>Advances in Neural
                            Information Processing Systems</em> (NeurIPS 2024).</li>
                    <li id="ref2">Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In
                        <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp.
                        7132-7141).</li>
                    <li id="ref3">Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by non-negative
                        matrix factorization. <em>Nature</em>, 401(6755), 788-791.</li>
                    <li id="ref4">Geng, Z., Guo, M. H., Chen, H., Li, X., Wei, K., & Lin, Z. (2021). Is attention better
                        than matrix decomposition? In <em>International Conference on Learning Representations</em>.
                    </li>
                    <li id="ref5">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &
                        Polosukhin, I. (2017). Attention is all you need. In <em>Advances in Neural Information
                            Processing Systems</em> (pp. 5998-6008).</li>
                    <li id="ref6">Yu, Z., Shen, Y., Shi, J., Zhao, H., Torr, P. H., & Zhao, G. (2022). PhysFormer:
                        Facial video-based physiological measurement with temporal difference transformer. In
                        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp.
                        4186-4196).</li>
                    <li id="ref7">Liu, X., Hill, B., Jiang, Z., Patel, S., & McDuff, D. (2023). EfficientPhys: Enabling
                        simple, fast and accurate camera-based cardiac measurement. In <em>Proceedings of the IEEE/CVF
                            Winter Conference on Applications of Computer Vision</em> (pp. 5008-5017).</li>
                    <li id="ref8">Stricker, R., Müller, S., & Gross, H. M. (2014). Non-contact video-based pulse rate
                        measurement on a mobile service robot. In <em>The 23rd IEEE International Symposium on Robot and
                            Human Interactive Communication</em> (pp. 1056-1062).</li>
                    <li id="ref9">McDuff, D., Wander, M., Liu, X., Hill, B., Hernandez, J., Lester, J., & Baltrusaitis,
                        T. (2022). SCAMPS: Synthetics for camera measurement of physiological signals. <em>Advances in
                            Neural Information Processing Systems</em>, 35, 3744-3757.</li>
                    <li id="ref10">Bobbia, S., Macwan, R., Benezeth, Y., Mansouri, A., & Dubois, J. (2019). Unsupervised
                        skin tissue segmentation for remote photoplethysmography. <em>Pattern Recognition Letters</em>,
                        124, 82-90.</li>
                </ol>
            </div>
        </section>
    </main>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Animate sections on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.animationDelay = '0.2s';
                    entry.target.classList.add('animated');
                }
            });
        }, observerOptions);

        document.querySelectorAll('.section').forEach(section => {
            observer.observe(section);
        });

        // Code syntax highlighting
        document.querySelectorAll('.code-block').forEach(block => {
            const content = block.innerHTML;
            // Simple syntax highlighting for comments
            const highlighted = content.replace(/(#.*$)/gm, '<span class="code-comment">$1</span>');
            block.innerHTML = highlighted;
        });
    </script>
</body>

</html>